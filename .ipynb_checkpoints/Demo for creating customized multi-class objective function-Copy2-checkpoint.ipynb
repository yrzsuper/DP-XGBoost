{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb32a456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== ['/Users/yangchengyi/Downloads/myCode/source_code/dpxgboost/lib/libxgboost.dylib']\n",
      "======== /Users/yangchengyi/opt/anaconda3/bin:/Users/yangchengyi/opt/anaconda3/condabin:/Library/Frameworks/Python.framework/Versions/2.7/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/usr/local/go/bin:/opt/X11/bin:/Library/Apple/usr/bin:/Users/yangchengyi/Downloads/myCode/source_code/dpxgboost/lib\n",
      "======== <CDLL '/Users/yangchengyi/Downloads/myCode/source_code/dpxgboost/lib/libxgboost.dylib', handle 206ff38e0 at 0x7f8f70323790>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dpxgboost as xgb\n",
    "from matplotlib import pyplot as plt\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2477bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e346853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kRows = 100\n",
    "kCols = 10\n",
    "kClasses = 4                    # number of classes\n",
    "\n",
    "kRounds = 10                    # number of boosting rounds.\n",
    "\n",
    "# Generate some random data for demo.\n",
    "X = np.random.randn(kRows, kCols)\n",
    "y = np.random.randint(0, 4, size=kRows)\n",
    "\n",
    "m = xgb.DMatrix(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9c2fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    '''Softmax function with x as input vector.'''\n",
    "    e = np.exp(x)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "\n",
    "def softprob_obj(predt: np.ndarray, data: xgb.DMatrix):\n",
    "    '''Loss function.  Computing the gradient and approximated hessian (diagonal).\n",
    "    Reimplements the `multi:softprob` inside XGBoost.\n",
    "\n",
    "    '''\n",
    "    print('======data======',type(data))\n",
    "    \n",
    "    labels = data.get_label()\n",
    "    if data.get_weight().size == 0:\n",
    "        # Use 1 as weight if we don't have custom weight.\n",
    "        weights = np.ones((kRows, 1), dtype=float)\n",
    "    else:\n",
    "        weights = data.get_weight()\n",
    "\n",
    "    # The prediction is of shape (rows, classes), each element in a row\n",
    "    # represents a raw prediction (leaf weight, hasn't gone through softmax\n",
    "    # yet).  In XGBoost 1.0.0, the prediction is transformed by a softmax\n",
    "    # function, fixed in later versions.\n",
    "    assert predt.shape == (kRows, kClasses)\n",
    "\n",
    "    grad = np.zeros((kRows, kClasses), dtype=float)\n",
    "    hess = np.zeros((kRows, kClasses), dtype=float)\n",
    "\n",
    "    eps = 1e-6\n",
    "\n",
    "    # compute the gradient and hessian, slow iterations in Python, only\n",
    "    # suitable for demo.  Also the one in native XGBoost core is more robust to\n",
    "    # numeric overflow as we don't do anything to mitigate the `exp` in\n",
    "    # `softmax` here.\n",
    "    for r in range(predt.shape[0]):\n",
    "        target = labels[r]\n",
    "        p = softmax(predt[r, :])\n",
    "        for c in range(predt.shape[1]):\n",
    "            assert target >= 0 or target <= kClasses\n",
    "            g = p[c] - 1.0 if c == target else p[c]\n",
    "            g = g * weights[r]\n",
    "            h = max((2.0 * p[c] * (1.0 - p[c]) * weights[r]).item(), eps)\n",
    "            grad[r, c] = g\n",
    "            hess[r, c] = h\n",
    "\n",
    "    # Right now (XGBoost 1.0.0), reshaping is necessary\n",
    "    grad = grad.reshape((kRows * kClasses, 1))\n",
    "    hess = hess.reshape((kRows * kClasses, 1))\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44e0f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(booster: xgb.Booster, X):\n",
    "    '''A customized prediction function that converts raw prediction to\n",
    "    target class.\n",
    "\n",
    "    '''\n",
    "    # Output margin means we want to obtain the raw prediction obtained from\n",
    "    # tree leaf weight.\n",
    "    predt = booster.predict(X, output_margin=True)\n",
    "    out = np.zeros(kRows)\n",
    "    for r in range(predt.shape[0]):\n",
    "        # the class with maximum prob (not strictly prob as it haven't gone\n",
    "        # through softmax yet so it doesn't sum to 1, but result is the same\n",
    "        # for argmax).\n",
    "        i = np.argmax(predt[r])\n",
    "        out[r] = i\n",
    "    return out\n",
    "\n",
    "\n",
    "def merror(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    y = dtrain.get_label()\n",
    "    # Like custom objective, the predt is untransformed leaf weight when custom objective\n",
    "    # is provided.\n",
    "\n",
    "    # With the use of `custom_metric` parameter in train function, custom metric receives\n",
    "    # raw input only when custom objective is also being used.  Otherwise custom metric\n",
    "    # will receive transformed prediction.\n",
    "    assert predt.shape == (kRows, kClasses)\n",
    "    out = np.zeros(kRows)\n",
    "    for r in range(predt.shape[0]):\n",
    "        i = np.argmax(predt[r])\n",
    "        out[r] = i\n",
    "\n",
    "    assert y.shape == out.shape\n",
    "\n",
    "    errors = np.zeros(kRows)\n",
    "    errors[y != out] = 1.0\n",
    "    return 'PyMError', np.sum(errors) / kRows\n",
    "\n",
    "\n",
    "def plot_history(custom_results, native_results):\n",
    "    fig, axs = plt.subplots(2, 1)\n",
    "    ax0 = axs[0]\n",
    "    ax1 = axs[1]\n",
    "\n",
    "    pymerror = custom_results['train']['PyMError']\n",
    "    merror = native_results['train']['merror']\n",
    "\n",
    "    x = np.arange(0, kRounds, 1)\n",
    "    ax0.plot(x, pymerror, label='Custom objective')\n",
    "    ax0.legend()\n",
    "    ax1.plot(x, merror, label='multi:softmax')\n",
    "    ax1.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384b0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    custom_results = {}\n",
    "    # Use our custom objective function\n",
    "    booster_custom = xgb.train({'num_class': kClasses,\n",
    "                                'disable_default_eval_metric': True},\n",
    "                               m,\n",
    "                               num_boost_round=kRounds,\n",
    "                               obj=softprob_obj,\n",
    "                               custom_metric=merror,\n",
    "                               evals_result=custom_results,\n",
    "                               evals=[(m, 'train')])\n",
    "\n",
    "    predt_custom = predict(booster_custom, m)\n",
    "    \n",
    "\n",
    "    native_results = {}\n",
    "    # Use the same objective function defined in XGBoost.\n",
    "    booster_native = xgb.train({'num_class': kClasses,\n",
    "                                \"objective\": \"multi:softmax\",\n",
    "                                'eval_metric': 'merror'},\n",
    "                               m,\n",
    "                               num_boost_round=kRounds,\n",
    "                               evals_result=native_results,\n",
    "                               evals=[(m, 'train')])\n",
    "    predt_native = booster_native.predict(m)\n",
    "\n",
    "    # We are reimplementing the loss function in XGBoost, so it should\n",
    "    # be the same for normal cases.\n",
    "    assert np.all(predt_custom == predt_native)\n",
    "    np.testing.assert_allclose(custom_results['train']['PyMError'],\n",
    "                               native_results['train']['merror'])\n",
    "\n",
    "    if args.plot != 0:\n",
    "        plot_history(custom_results, native_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eab809",
   "metadata": {},
   "source": [
    "dir(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644c0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3c156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f6045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84054020",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Arguments for custom softmax objective function demo.')\n",
    "    parser.add_argument(\n",
    "        '--plot',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='Set to 0 to disable plotting the evaluation history.')\n",
    "    args = parser.parse_args(args=[]) \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec10870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded75831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c3c2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
